{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16300b0a-6c5e-4cb8-a19c-fcbf0ec10d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Memulai SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Simple DataFrame\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf0b0cbe-2012-402f-a441-b7605994af57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------+\n",
      "|EmployeeName|Department|Salary|\n",
      "+------------+----------+------+\n",
      "|       James|     Sales|  3000|\n",
      "|     Michael|     Sales|  4600|\n",
      "|      Robert|     Sales|  4100|\n",
      "|       Maria|   Finance|  3000|\n",
      "+------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark \n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession object\n",
    "spark = SparkSession.builder.appName(\"CreateDataFrame\").getOrCreate()\n",
    "\n",
    "# Use the SparkSession object to create a DataFrame\n",
    "data = spark.createDataFrame([('James', 'Sales', 3000),\n",
    "                              ('Michael', 'Sales', 4600),\n",
    "                              ('Robert', 'Sales', 4100),\n",
    "                              ('Maria', 'Finance', 3000)],\n",
    "                             ['EmployeeName', 'Department', 'Salary'])\n",
    "# Show the DataFrame\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf82335-00ff-44d4-b137-ecdba5a14663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7269111-13a8-44d4-a2c8-3b5e37761d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------+\n",
      "|EmployeeName|Department|Salary|\n",
      "+------------+----------+------+\n",
      "|       James|     Sales|  3000|\n",
      "|     Michael|     Sales|  4600|\n",
      "|      Robert|     Sales|  4100|\n",
      "|       Maria|   Finance|  3000|\n",
      "+------------+----------+------+\n",
      "\n",
      "Sales Department Employees:\n",
      "+------------+----------+------+\n",
      "|EmployeeName|Department|Salary|\n",
      "+------------+----------+------+\n",
      "|       James|     Sales|  3000|\n",
      "|     Michael|     Sales|  4600|\n",
      "|      Robert|     Sales|  4100|\n",
      "+------------+----------+------+\n",
      "\n",
      "Selected Columns (EmployeeName and Salary):\n",
      "+------------+------+\n",
      "|EmployeeName|Salary|\n",
      "+------------+------+\n",
      "|       James|  3000|\n",
      "|     Michael|  4600|\n",
      "|      Robert|  4100|\n",
      "|       Maria|  3000|\n",
      "+------------+------+\n",
      "\n",
      "Aggregated Data by Department:\n",
      "+----------+-------------+---------+-----------+\n",
      "|Department|AverageSalary|MaxSalary|TotalSalary|\n",
      "+----------+-------------+---------+-----------+\n",
      "|     Sales|       3900.0|     4600|      11700|\n",
      "|   Finance|       3000.0|     3000|       3000|\n",
      "+----------+-------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import mean, max, sum\n",
    "\n",
    "# Create a SparkSession object\n",
    "spark = SparkSession.builder.appName(\"CreateDataFrame\").getOrCreate()\n",
    "\n",
    "# Use the SparkSession object to create a DataFrame\n",
    "data = spark.createDataFrame([('James', 'Sales', 3000),\n",
    "                              ('Michael', 'Sales', 4600),\n",
    "                              ('Robert', 'Sales', 4100),\n",
    "                              ('Maria', 'Finance', 3000)],\n",
    "                             ['EmployeeName', 'Department', 'Salary'])\n",
    "\n",
    "# Show the DataFrame\n",
    "data.show()\n",
    "\n",
    "# Filter data to only show employees in the Sales department\n",
    "sales_department = data.filter(data.Department == 'Sales')\n",
    "print(\"Sales Department Employees:\")\n",
    "sales_department.show()\n",
    "\n",
    "# Select specific columns (EmployeeName and Salary)\n",
    "selected_columns = data.select('EmployeeName', 'Salary')\n",
    "print(\"Selected Columns (EmployeeName and Salary):\")\n",
    "selected_columns.show()\n",
    "\n",
    "# Group data by Department and calculate aggregate functions (mean, max, sum)\n",
    "aggregated_data = data.groupBy('Department').agg(\n",
    "    mean('Salary').alias('AverageSalary'),\n",
    "    max('Salary').alias('MaxSalary'),\n",
    "    sum('Salary').alias('TotalSalary')\n",
    ")\n",
    "\n",
    "print(\"Aggregated Data by Department:\")\n",
    "aggregated_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1abd4a2-8ab0-480f-824a-e47e04748096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|ID |Name              |\n",
      "+---+------------------+\n",
      "|1  |{James, Smith}    |\n",
      "|2  |{Anna, Rose}      |\n",
      "|3  |{Robert, Williams}|\n",
      "+---+------------------+\n",
      "\n",
      "+---------+--------+\n",
      "|FirstName|LastName|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|     Anna|    Rose|\n",
      "|   Robert|Williams|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"ComplexTypes\").getOrCreate()\n",
    "\n",
    "# Create DataFrame with StructType\n",
    "data = [(1, (\"James\", \"Smith\")), \n",
    "        (2, (\"Anna\", \"Rose\")),\n",
    "        (3, (\"Robert\", \"Williams\"))]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StructType([\n",
    "        StructField(\"FirstName\", StringType(), True),\n",
    "        StructField(\"LastName\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Accessing nested columns\n",
    "df.select(\"Name.FirstName\", \"Name.LastName\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60294c5c-99bf-4876-b886-8b79125073e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|ID |Fruits                 |\n",
      "+---+-----------------------+\n",
      "|1  |[apple, banana, cherry]|\n",
      "|2  |[orange, grape]        |\n",
      "|3  |[watermelon]           |\n",
      "+---+-----------------------+\n",
      "\n",
      "+---+----------+\n",
      "| ID|     Fruit|\n",
      "+---+----------+\n",
      "|  1|     apple|\n",
      "|  1|    banana|\n",
      "|  1|    cherry|\n",
      "|  2|    orange|\n",
      "|  2|     grape|\n",
      "|  3|watermelon|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Create DataFrame with ArrayType\n",
    "data = [(1, [\"apple\", \"banana\", \"cherry\"]),\n",
    "        (2, [\"orange\", \"grape\"]),\n",
    "        (3, [\"watermelon\"])]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"Fruits\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Explode array to rows\n",
    "from pyspark.sql.functions import explode\n",
    "df.select(\"ID\", explode(\"Fruits\").alias(\"Fruit\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "609f3ee4-9f59-45c0-b123-87dec1904f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------+\n",
      "|ID |FruitColors                        |\n",
      "+---+-----------------------------------+\n",
      "|1  |{banana -> yellow, apple -> red}   |\n",
      "|2  |{orange -> orange, grape -> purple}|\n",
      "+---+-----------------------------------+\n",
      "\n",
      "+---+-----+------+\n",
      "| ID|apple|banana|\n",
      "+---+-----+------+\n",
      "|  1|  red|yellow|\n",
      "|  2| NULL|  NULL|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import MapType, StringType\n",
    "\n",
    "# Create DataFrame with MapType\n",
    "data = [(1, {\"apple\": \"red\", \"banana\": \"yellow\"}),\n",
    "        (2, {\"orange\": \"orange\", \"grape\": \"purple\"})]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"FruitColors\", MapType(StringType(), StringType()), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Access value by key from Map\n",
    "df.select(\"ID\", \"FruitColors.apple\", \"FruitColors.banana\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04e5cf9e-3fac-4855-81c8-29ae9f780e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------+\n",
      "|EmployeeName|Department|Salary|\n",
      "+------------+----------+------+\n",
      "|       James|     Sales|  3000|\n",
      "|     Michael|     Sales|  4600|\n",
      "|      Robert|     Sales|  4100|\n",
      "|       Maria|   Finance|  3000|\n",
      "|       James|     Sales|  2000|\n",
      "|       Scott|   Finance|  3300|\n",
      "|         Jen| Marketing|  3900|\n",
      "|        Jeff| Marketing|  3000|\n",
      "|       Kumar| Marketing|  2000|\n",
      "+------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum, rank\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"WindowFunctions\").getOrCreate()\n",
    "\n",
    "# Sample data for sales\n",
    "data = [(\"James\", \"Sales\", 3000),\n",
    "        (\"Michael\", \"Sales\", 4600),\n",
    "        (\"Robert\", \"Sales\", 4100),\n",
    "        (\"Maria\", \"Finance\", 3000),\n",
    "        (\"James\", \"Sales\", 2000),\n",
    "        (\"Scott\", \"Finance\", 3300),\n",
    "        (\"Jen\", \"Marketing\", 3900),\n",
    "        (\"Jeff\", \"Marketing\", 3000),\n",
    "        (\"Kumar\", \"Marketing\", 2000)]\n",
    "\n",
    "columns = [\"EmployeeName\", \"Department\", \"Salary\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59060d2c-6fde-4124-b97c-b60b428bcb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------+------------+\n",
      "|EmployeeName|Department|Salary|RunningTotal|\n",
      "+------------+----------+------+------------+\n",
      "|       Maria|   Finance|  3000|        3000|\n",
      "|       Scott|   Finance|  3300|        6300|\n",
      "|       Kumar| Marketing|  2000|        2000|\n",
      "|        Jeff| Marketing|  3000|        5000|\n",
      "|         Jen| Marketing|  3900|        8900|\n",
      "|       James|     Sales|  2000|        2000|\n",
      "|       James|     Sales|  3000|        5000|\n",
      "|      Robert|     Sales|  4100|        9100|\n",
      "|     Michael|     Sales|  4600|       13700|\n",
      "+------------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Define the window specification\n",
    "windowSpec = Window.partitionBy(\"Department\").orderBy(\"Salary\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Calculate running total\n",
    "df.withColumn(\"RunningTotal\", sum(col(\"Salary\")).over(windowSpec)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea7bc323-aa27-4a2f-a8ed-7372c3d08ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------+----+\n",
      "|EmployeeName|Department|Salary|Rank|\n",
      "+------------+----------+------+----+\n",
      "|       Scott|   Finance|  3300|   1|\n",
      "|       Maria|   Finance|  3000|   2|\n",
      "|         Jen| Marketing|  3900|   1|\n",
      "|        Jeff| Marketing|  3000|   2|\n",
      "|       Kumar| Marketing|  2000|   3|\n",
      "|     Michael|     Sales|  4600|   1|\n",
      "|      Robert|     Sales|  4100|   2|\n",
      "|       James|     Sales|  3000|   3|\n",
      "|       James|     Sales|  2000|   4|\n",
      "+------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "\n",
    "# Define the window specification for ranking\n",
    "windowSpecRank = Window.partitionBy(\"Department\").orderBy(col(\"Salary\").desc())\n",
    "\n",
    "# Calculate rank\n",
    "df.withColumn(\"Rank\", rank().over(windowSpecRank)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18dcee74-59c0-499f-bd3c-aec83e5f2b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------+------------+----+\n",
      "|EmployeeName|Department|Salary|RunningTotal|Rank|\n",
      "+------------+----------+------+------------+----+\n",
      "|       Scott|   Finance|  3300|        6300|   1|\n",
      "|       Maria|   Finance|  3000|        3000|   2|\n",
      "|         Jen| Marketing|  3900|        8900|   1|\n",
      "|        Jeff| Marketing|  3000|        5000|   2|\n",
      "|       Kumar| Marketing|  2000|        2000|   3|\n",
      "|     Michael|     Sales|  4600|       13700|   1|\n",
      "|      Robert|     Sales|  4100|        9100|   2|\n",
      "|       James|     Sales|  3000|        5000|   3|\n",
      "|       James|     Sales|  2000|        2000|   4|\n",
      "+------------+----------+------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running total and rank together\n",
    "df.withColumn(\"RunningTotal\", sum(col(\"Salary\")).over(windowSpec)) \\\n",
    "  .withColumn(\"Rank\", rank().over(windowSpecRank)).show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
